{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d601dbd6",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94481535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sender</th>\n",
       "      <th>receiver</th>\n",
       "      <th>date</th>\n",
       "      <th>subject</th>\n",
       "      <th>body</th>\n",
       "      <th>label</th>\n",
       "      <th>urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Robert Elz &lt;kre@munnari.OZ.AU&gt;</td>\n",
       "      <td>Chris Garrigues &lt;cwg-dated-1030377287.06fa6d@D...</td>\n",
       "      <td>Thu, 22 Aug 2002 18:26:25 +0700</td>\n",
       "      <td>Re: New Sequences Window</td>\n",
       "      <td>Date:        Wed, 21 Aug 2002 10:54:46 -0500  ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Steve Burt &lt;Steve_Burt@cursor-system.com&gt;</td>\n",
       "      <td>\"'zzzzteana@yahoogroups.com'\" &lt;zzzzteana@yahoo...</td>\n",
       "      <td>Thu, 22 Aug 2002 12:46:18 +0100</td>\n",
       "      <td>[zzzzteana] RE: Alexander</td>\n",
       "      <td>Martin A posted:\\nTassos Papadopoulos, the Gre...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Tim Chapman\" &lt;timc@2ubh.com&gt;</td>\n",
       "      <td>zzzzteana &lt;zzzzteana@yahoogroups.com&gt;</td>\n",
       "      <td>Thu, 22 Aug 2002 13:52:38 +0100</td>\n",
       "      <td>[zzzzteana] Moscow bomber</td>\n",
       "      <td>Man Threatens Explosion In Moscow \\n\\nThursday...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Monty Solomon &lt;monty@roscom.com&gt;</td>\n",
       "      <td>undisclosed-recipient: ;</td>\n",
       "      <td>Thu, 22 Aug 2002 09:15:25 -0400</td>\n",
       "      <td>[IRR] Klez: The Virus That  Won't Die</td>\n",
       "      <td>Klez: The Virus That Won't Die\\n \\nAlready the...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stewart Smith &lt;Stewart.Smith@ee.ed.ac.uk&gt;</td>\n",
       "      <td>zzzzteana@yahoogroups.com</td>\n",
       "      <td>Thu, 22 Aug 2002 14:38:22 +0100</td>\n",
       "      <td>Re: [zzzzteana] Nothing like mama used to make</td>\n",
       "      <td>&gt;  in adding cream to spaghetti carbonara, whi...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      sender  \\\n",
       "0             Robert Elz <kre@munnari.OZ.AU>   \n",
       "1  Steve Burt <Steve_Burt@cursor-system.com>   \n",
       "2              \"Tim Chapman\" <timc@2ubh.com>   \n",
       "3           Monty Solomon <monty@roscom.com>   \n",
       "4  Stewart Smith <Stewart.Smith@ee.ed.ac.uk>   \n",
       "\n",
       "                                            receiver  \\\n",
       "0  Chris Garrigues <cwg-dated-1030377287.06fa6d@D...   \n",
       "1  \"'zzzzteana@yahoogroups.com'\" <zzzzteana@yahoo...   \n",
       "2              zzzzteana <zzzzteana@yahoogroups.com>   \n",
       "3                           undisclosed-recipient: ;   \n",
       "4                          zzzzteana@yahoogroups.com   \n",
       "\n",
       "                              date  \\\n",
       "0  Thu, 22 Aug 2002 18:26:25 +0700   \n",
       "1  Thu, 22 Aug 2002 12:46:18 +0100   \n",
       "2  Thu, 22 Aug 2002 13:52:38 +0100   \n",
       "3  Thu, 22 Aug 2002 09:15:25 -0400   \n",
       "4  Thu, 22 Aug 2002 14:38:22 +0100   \n",
       "\n",
       "                                          subject  \\\n",
       "0                        Re: New Sequences Window   \n",
       "1                       [zzzzteana] RE: Alexander   \n",
       "2                       [zzzzteana] Moscow bomber   \n",
       "3           [IRR] Klez: The Virus That  Won't Die   \n",
       "4  Re: [zzzzteana] Nothing like mama used to make   \n",
       "\n",
       "                                                body  label  urls  \n",
       "0  Date:        Wed, 21 Aug 2002 10:54:46 -0500  ...      0     1  \n",
       "1  Martin A posted:\\nTassos Papadopoulos, the Gre...      0     1  \n",
       "2  Man Threatens Explosion In Moscow \\n\\nThursday...      0     1  \n",
       "3  Klez: The Virus That Won't Die\\n \\nAlready the...      0     1  \n",
       "4  >  in adding cream to spaghetti carbonara, whi...      0     1  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import re  # Regular expressions\n",
    "import nltk  # Natural Language Toolkit\n",
    "import os  # Operating system utilities (for file paths)\n",
    "from bs4 import BeautifulSoup  # For parsing HTML\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Define the path to the dataset\n",
    "file_dir = \"../DS/Assassin.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(f\"{file_dir}\")\n",
    "\n",
    "# Display the first 5 rows to understand the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5c909e",
   "metadata": {},
   "source": [
    "## Part 1: Basic Text Preprocessing\n",
    "\n",
    "- Only keep colums for the email's body text and the label\n",
    "- Drop rows with duplicated or missing values\n",
    "- Clean email body text by \n",
    "    - Extracting plain text\n",
    "    - Normalizing whitespace\n",
    "    - Removing emails, URLs, and punctuations\n",
    "    - Lowercasing words and stripping leading/trailing spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2301836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the 'body' (email text) and 'label' (spam/not spam)\n",
    "df = df[['body', 'label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9c3525",
   "metadata": {},
   "source": [
    "### Data Integrity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64476e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after dropna():\n",
      "body     0\n",
      "label    0\n",
      "dtype: int64\n",
      "\n",
      "Remaining duplicates: 0\n"
     ]
    }
   ],
   "source": [
    "# 1. Remove rows with missing values (NaN)\n",
    "df = df.dropna()\n",
    "print(\"Missing values after dropna():\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# 2. Remove rows with duplicated email bodies\n",
    "df = df.drop_duplicates(subset=['body'])\n",
    "print(f\"\\nRemaining duplicates: {df.duplicated(subset=['body']).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7110c9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_30196\\1898847667.py:11: MarkupResemblesLocatorWarning: The input passed in on this line looks more like a URL than HTML or XML.\n",
      "\n",
      "If you meant to use Beautiful Soup to parse the web page found at a certain URL, then something has gone wrong. You should use an Python package like 'requests' to fetch the content behind the URL. Once you have the content as a string, you can feed that string into Beautiful Soup.\n",
      "\n",
      "However, if you want to parse some data that happens to look like a URL, then nothing has gone wrong: you are using Beautiful Soup correctly, and this warning is spurious and can be filtered. To make this warning go away, run this code before calling the BeautifulSoup constructor:\n",
      "\n",
      "    from bs4 import MarkupResemblesLocatorWarning\n",
      "    import warnings\n",
      "\n",
      "    warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
      "    \n",
      "  soup = BeautifulSoup(html_text, 'html.parser')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after basic cleaning:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_email_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>date  wed     aug                     from  ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>martin a posted  tassos papadopoulos  the gree...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>man threatens explosion in moscow thursday aug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>klez  the virus that won t die already the mos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>in adding cream to spaghetti carbonara  which ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                 cleaned_email_body\n",
       "0      0  date  wed     aug                     from  ch...\n",
       "1      0  martin a posted  tassos papadopoulos  the gree...\n",
       "2      0  man threatens explosion in moscow thursday aug...\n",
       "3      0  klez  the virus that won t die already the mos...\n",
       "4      0  in adding cream to spaghetti carbonara  which ..."
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_email_body(html_text):\n",
    "    \"\"\"\n",
    "    Cleans raw email text by:\n",
    "    1. Parsing HTML and extracting plain text.\n",
    "    2. Normalizing whitespace (newlines, tabs, etc.).\n",
    "    3. Removing URLs, email addresses, and all non-alphabetic characters.\n",
    "    4. Lowercasing and stripping final whitespace.\n",
    "    \"\"\"\n",
    "    # 1. Parse HTML and extract plain text\n",
    "    try:\n",
    "        soup = BeautifulSoup(html_text, 'html.parser')\n",
    "        text = soup.get_text()\n",
    "    except:\n",
    "        # Handle cases where the body might not be HTML (e.g., just plain text)\n",
    "        # Ensure the input is treated as a string\n",
    "        text = str(html_text)\n",
    "\n",
    "    # 2. Normalize whitespace (replace multiple spaces, newlines, tabs with a single space)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # 3. Remove noise: URLs, emails, and punctuation\n",
    "    text = re.sub(r'http\\S+', ' ', text)      # Replace URLs with a space\n",
    "    text = re.sub(r'\\S+@\\S+', ' ', text)     # Replace emails with a space\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text) # Replace non-letters/non-spaces with a space\n",
    "\n",
    "    # 4. Final cleanup: lowercase and strip leading/trailing spaces\n",
    "    text = text.lower().strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function to the 'body' column\n",
    "df[\"cleaned_email_body\"] = df[\"body\"].apply(clean_email_body)\n",
    "\n",
    "# Drop the original, raw 'body' column as it's no longer needed\n",
    "df = df.drop(columns=\"body\")\n",
    "\n",
    "print(\"\\nDataFrame after basic cleaning:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04d7502",
   "metadata": {},
   "source": [
    "## Part 2: NLP Processing\n",
    "\n",
    "- Downloaded the necessary NLTK packages (punkt, stopwords, wordnet).\n",
    "- Initialized the `WordNetLemmatizer` and created a set of English `stopwords` for fast lookup.\n",
    "- Defined a function (`preprocess_text_2`) that:\n",
    "    - Tokenized the clean text (split it into a list of words).\n",
    "    - Looped through the list and kept only words that were not in the stopword list.\n",
    "    - Lemmatized each of those remaining words.\n",
    "    - Returned `None` if the text became empty (e.g., it only contained stopwords).\n",
    "    - Joined the final list of processed words back into a single string.\n",
    "- Applied this function to the `cleaned_email_body` column.\n",
    "- Stored the output in a new `final_text` column.\n",
    "- Dropped the intermediate `cleaned_email_body` column.\n",
    "- Dropped any rows that was null from the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd291d10",
   "metadata": {},
   "source": [
    "### Download NLTK Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e021be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These only need to be downloaded once.\n",
    "nltk.download('punkt')      # For the tokenizer\n",
    "nltk.download('punkt_tab')  # Additional tokenizer resource\n",
    "nltk.download('stopwords')  # For the list of stopwords\n",
    "nltk.download('wordnet')    # For the lemmatizer\n",
    "nltk.download('omw-1.4')    # Additional lemmatizer resource\n",
    "nltk.download('words')      # For English vocabulary (less common, but good to have)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d713f67",
   "metadata": {},
   "source": [
    "### Initialize NLP Tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae616991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stopwords loaded: 198\n",
      "Tokenizer test: ['hello', 'world', '!']\n",
      "\n",
      "Missing values per column after NLP processing:\n",
      "label         0\n",
      "final_text    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>final_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>date wed aug chris garrigues message id reprod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>martin posted tasso papadopoulos greek sculpto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>man threatens explosion moscow thursday august...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>klez virus die already prolific virus ever kle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>adding cream spaghetti carbonara effect pasta ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                         final_text\n",
       "0      0  date wed aug chris garrigues message id reprod...\n",
       "1      0  martin posted tasso papadopoulos greek sculpto...\n",
       "2      0  man threatens explosion moscow thursday august...\n",
       "3      0  klez virus die already prolific virus ever kle...\n",
       "4      0  adding cream spaghetti carbonara effect pasta ..."
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load stopwords into a set for faster lookup\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(f\"\\nStopwords loaded: {len(stop_words)}\")\n",
    "print(f\"Tokenizer test: {word_tokenize('hello world!')}\")\n",
    "\n",
    "def preprocess_text_2(text, min_words=1):\n",
    "    \"\"\"\n",
    "    Applies tokenization, stopword removal, and lemmatization.\n",
    "    \"\"\"\n",
    "    # 1. Tokenize: Split text into a list of words\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    processed_tokens = []\n",
    "    for word in tokens:\n",
    "        # 2. Filter: Remove stopwords and keep only alphabetic words\n",
    "        if word not in stop_words:\n",
    "            # 3. Lemmatize: Reduce word to its root form\n",
    "            processed_tokens.append(lemmatizer.lemmatize(word))\n",
    "\n",
    "    # 4. Optional: Skip texts that become empty after processing\n",
    "    # (e.g., an email that only contained stopwords)\n",
    "    if len(processed_tokens) < min_words:\n",
    "        return None # This will be dropped as NaN later\n",
    "\n",
    "    # 5. Re-join the processed tokens into a single string\n",
    "    return \" \".join(processed_tokens)\n",
    "\n",
    "# Apply the advanced NLP function\n",
    "df['final_text'] = df['cleaned_email_body'].apply(preprocess_text_2)\n",
    "\n",
    "# Drop the intermediate 'cleaned_email_body' column\n",
    "df = df.drop(columns=\"cleaned_email_body\")\n",
    "\n",
    "# Drop any rows that became empty (NaN) during the NLP step\n",
    "df = df.dropna()\n",
    "\n",
    "print(\"\\nMissing values per column after NLP processing:\")\n",
    "print(df.isna().sum())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1417e2ff",
   "metadata": {},
   "source": [
    "### DataFrame Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "08d05d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final formatted DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>final_text</th>\n",
       "      <th>is_spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>date wed aug chris garrigues message id reprod...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>martin posted tasso papadopoulos greek sculpto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>man threatens explosion moscow thursday august...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>klez virus die already prolific virus ever kle...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>adding cream spaghetti carbonara effect pasta ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          final_text  is_spam\n",
       "0  date wed aug chris garrigues message id reprod...        0\n",
       "1  martin posted tasso papadopoulos greek sculpto...        0\n",
       "2  man threatens explosion moscow thursday august...        0\n",
       "3  klez virus die already prolific virus ever kle...        0\n",
       "4  adding cream spaghetti carbonara effect pasta ...        0"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename 'label' to 'is_spam' for better readability\n",
    "df.rename(columns={'label': 'is_spam'}, inplace=True)\n",
    "\n",
    "# Reorder columns: feature (X) first, target (y) second\n",
    "df = df[['final_text', 'is_spam']]\n",
    "\n",
    "# Reset the DataFrame index after dropping rows\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "print(\"\\nFinal formatted DataFrame:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2462ba14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- File Saving Utility ---\n",
    "\n",
    "def get_filename_without_ext(file_dir):\n",
    "    \"\"\"\n",
    "    Gets the filename (e.g., \"Ling\") without its extension\n",
    "    from a given path (e.g., \"../DS/Ling.csv\").\n",
    "    \"\"\"\n",
    "    # 1. Get the full filename (e.g., \"Ling.csv\")\n",
    "    filename = os.path.basename(file_dir)\n",
    "    # 2. Split the filename from its extension and return just the name\n",
    "    filename_without_ext = os.path.splitext(filename)[0]\n",
    "    return filename_without_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6b9d386c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully cleaned and saved data to: ../cleaned_DS/Assassin_cleaned.csv\n",
      "Total processed emails: 5800\n"
     ]
    }
   ],
   "source": [
    "# --- Save Cleaned Data ---\n",
    "\n",
    "# Define the output directory\n",
    "output_dir = \"../cleaned_DS/\"\n",
    "\n",
    "# Create the output directory if it doesn't already exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Generate the new filename\n",
    "filename_without_ext = get_filename_without_ext(file_dir)\n",
    "output_path = f\"{output_dir}{filename_without_ext}_cleaned.csv\"\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV, without the pandas index\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\nSuccessfully cleaned and saved data to: {output_path}\")\n",
    "print(f\"Total processed emails: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039c88c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
